{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Deployable Function Notebook for RAG-Grounded Chat\nThis notebook contains steps and code to test, promote, and deploy a Python function\ncapturing logic to implement RAG pattern for grounded chats. It introduces Python API commands\nfor authentication using API key and prompt inferencing using WML API.\n\n**Note:** Notebook code generated using Prompt Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Prompt Lab as a notebook.</a>\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n## Contents\nThis notebook contains the following parts:\n\n1. Setup\n2. Get an ID for a function deployment\n3. Initialize all the variables needed by the function\n4. Create a deployable function\n5. Test the deployed function\n\n## 1. Set up the environment\n\nBefore you can run this notebook, you must perform the following setup tasks:"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Connection to WML\nThis cell defines the credentials required to work with watsonx API for both the execution in the project, \nas well as the deployment and runtime execution of the function.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\nimport getpass\nimport requests\n\ndef get_credentials():\n    return {\n        \"url\" : \"https://us-south.ml.cloud.ibm.com\",\n        \"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n    }\n\ndef get_bearer_token():\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/86488f09-6626-47f0-8f6c-736cc3541afc?projectid=b43a5e28-8c49-4cc1-94e5-ce92969f6217&context=cpdaas#\n    data = f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={credentials['apikey']}\"\n\n    response = requests.post(url, headers=headers, data=data)\n    return response.json().get(\"access_token\")\n\ncredentials = get_credentials()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from ibm_watsonx_ai import APIClient\n\nclient = APIClient(credentials)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Connecting to a space\nA space will be be used to host the promoted function."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "space_id = \"a06ae04e-5905-41af-b9cd-f097e02e1f63\"\nclient.set.default_space(space_id)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Promote asset(s) to space\nWe will now promote assets we will need to stage in the space so that we can access their data from the deployed function."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "promoted_assets = {}\npromoted_assets[\"vector_index_id\"] = client.spaces.promote(\"61fe620d-db36-4a6a-89c6-4c093a1b3e47\", \"b43a5e28-8c49-4cc1-94e5-ce92969f6217\", \"a06ae04e-5905-41af-b9cd-f097e02e1f63\")\nprint(promoted_assets)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 2. Create a deployable interactive chain function\nWe first need to define the chain function.\n\n### 2.1 Define the function"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "deploy_credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"token\": get_bearer_token()\n}\n\nai_params = {\"credentials\": credentials, \"space_id\": space_id, \"promoted_assets\": promoted_assets}\n\ndef my_deployable_chain_function( params=ai_params ):\n    import subprocess\n    from ibm_watsonx_ai.foundation_models import Model\n    from ibm_watsonx_ai import APIClient\n\n    client = APIClient(params[\"credentials\"])\n    space_id = \"a06ae04e-5905-41af-b9cd-f097e02e1f63\"\n    client.set.default_space(space_id)\n    \n    vector_index_details = client.data_assets.get_details(params[\"promoted_assets\"][\"vector_index_id\"])\n    vector_index_properties = vector_index_details[\"entity\"][\"vector_index\"]\n\n    import gzip\n    import json\n    import chromadb\n    import random\n    import string\n    from ibm_watsonx_ai.foundation_models.embeddings.sentence_transformer_embeddings import SentenceTransformerEmbeddings\n\n    emb = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n\n    vector_index_id = params[\"promoted_assets\"][\"vector_index_id\"]\n    vector_index_details = client.data_assets.get_details(vector_index_id)\n    vector_index_properties = vector_index_details[\"entity\"][\"vector_index\"]\n\n    def hydrate_chromadb():\n        data = client.data_assets.get_content(vector_index_id)\n        content = gzip.decompress(data)\n        stringified_vectors = str(content, \"utf-8\")\n        vectors = json.loads(stringified_vectors)\n        \n        chroma_client = chromadb.Client()\n        \n        # make sure collection is empty if it already existed\n        collection_name = \"my_collection\"\n        try:\n            collection = chroma_client.delete_collection(name=collection_name)\n        except:\n            print(\"Collection didn't exist - nothing to do.\")\n        collection = chroma_client.create_collection(name=collection_name)\n        \n        vector_embeddings = []\n        vector_documents = []\n        vector_metadatas = []\n        vector_ids = []\n        \n        for vector in vectors:\n            vector_embeddings.append(vector[\"embedding\"])\n            vector_documents.append(vector[\"content\"])\n            metadata = vector[\"metadata\"]\n            lines = metadata[\"loc\"][\"lines\"]\n            clean_metadata = {}\n            clean_metadata[\"asset_id\"] = metadata[\"asset_id\"]\n            clean_metadata[\"asset_name\"] = metadata[\"asset_name\"]\n            clean_metadata[\"url\"] = metadata[\"url\"]\n            clean_metadata[\"from\"] = lines[\"from\"]\n            clean_metadata[\"to\"] = lines[\"to\"]\n            vector_metadatas.append(clean_metadata)\n            asset_id = vector[\"metadata\"][\"asset_id\"]\n            random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n            id = \"{}:{}-{}-{}\".format(asset_id, lines[\"from\"], lines[\"to\"], random_string)\n            vector_ids.append(id)\n\n        collection.add(\n            embeddings=vector_embeddings,\n            documents=vector_documents,\n            metadatas=vector_metadatas,\n            ids=vector_ids\n        )\n        return collection\n    \n    chroma_collection = hydrate_chromadb()\n\n    def proximity_search( question ):\n        query_vectors = emb.embed_query(question)\n        query_result = chroma_collection.query(\n            query_embeddings=query_vectors,\n            n_results=vector_index_properties[\"settings\"][\"top_k\"],\n            include=[\"documents\", \"metadatas\", \"distances\"]\n        )\n        \n        documents = list(reversed(query_result[\"documents\"][0]))\n        metadatas = reversed(query_result[\"metadatas\"][0])\n        distances = reversed(query_result[\"distances\"][0])\n        results = []\n        for metadata, distance in zip(metadatas, distances):\n            results.append({\n                \"metadata\": metadata,\n                \"score\": distance\n            })\n\n        return {\n            \"results\": results,\n            \"documents\": documents\n        }\n\n    def format_input(messages, documents):\n        grounding = \"\\n\".join(documents)\n        system = f\"\"\"<|start_header_id|>system<|end_header_id|>\n\nYou are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. When the question cannot be answered using the context or document, output the following response: \"I cannot answer that question based on the provided document.\" Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer.\n\n\nWhenever you want to provide answer, make sure that you privide the response in the following format:\n\n     *** Issue title ***:  Subject\n     *** Conclusions ***: What are the key takeaways or conclusions from the discussion in the document, provide via bullet point?\n     *** Next Actions ***: What are the next steps that need to be taken in the document, provide via bullet point?\n     *** Key Stakeholders ***: Who are the main stakeholders and individuals involved in the document, provide personal name of stakeholders as well with their responsibility?\n     *** Related Risks ***: What risks or dependencies are associated with this issue in the document, provide via bullet point?\n     *** ETA ***: What is the estimated time for completion of the tasks in the document, provide via bullet point?<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{grounding}\"\"\"\n        messages_section = []\n\n        for index,value in enumerate(messages, start=0):\n            content = value[\"content\"]\n            user_template = f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{content}\"\"\"\n            assistant_template = f\"\"\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{content}\"\"\"\n            grounded_user_template = f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{content}\"\"\"\n\n            formatted_entry = user_template if value[\"role\"] == \"user\" else assistant_template\n            if (index == len(messages)-1):\n                formatted_entry = grounded_user_template\n            \n            messages_section.append(formatted_entry)\n\n        messages_section = \"\".join(messages_section)\n        prompt = f\"\"\"<|begin_of_text|>{system}{messages_section}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n        return prompt\n    \n    def inference_model( messages, documents, access_token ):\n        prompt = format_input(messages, documents)\n        model_id = \"meta-llama/llama-3-70b-instruct\"\n        parameters = {\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 900,\n            \"repetition_penalty\": 1\n        }\n        inference_credentials = {\n            \"url\": params[\"credentials\"].get(\"url\"),\n            \"token\": access_token\n        }\n        model = Model(\n            model_id = model_id,\n            params = parameters,\n            credentials = inference_credentials,\n            space_id = params[\"space_id\"]\n        )\n         # Generate grounded response\n        generated_response = model.generate_text(prompt=prompt, guardrails=False)\n        return generated_response\n\n    def execute( payload ):\n        messages = payload.get(\"input_data\")[0].get(\"values\")[0]\n        access_token = payload.get(\"input_data\")[0].get(\"values\")[1][0]\n \n        # Proximity search\n        search_result = proximity_search(messages[-1].get(\"content\"))\n        \n        # Grunded inferencing\n        generated_response = inference_model(messages, search_result[\"documents\"], access_token)\n        \n        execute_response = {\n            \"predictions\": [{\"fields\": [\"Proximity search result\", \"Generated response\"],\n                             \"values\": [search_result[\"results\"], generated_response]\n                             }]\n        }\n        return execute_response\n\n    return execute\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2.2 Test locally"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Initialize deployable chain function locally\n\nlocal_function = my_deployable_chain_function()\nmessages = []",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Chose to retain the history\nretain_history = False\nif (retain_history == False):\n    messages = []\n\nlocal_question = \"Change this question to test your function\"\n\nmessages.append({ \"role\" : \"user\", \"content\": local_question })\n\nfunc_result = local_function({\"input_data\": [{\"fields\": [ \"Search\" \"access_token\"],\n                                                              \"values\": [messages, [get_bearer_token()]]\n                                                             }\n                                                            ]\n                                             })\n\nresponse = func_result[\"predictions\"][0][\"values\"][1]\nmessages.append({\"role\": \"assistant\", \"content\": response })\nprint(func_result)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 3. Store and deploy the function\nBefore you can deploy the function, you must store the function in your watsonx.ai repository."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Look up software specification for the deployable function\n\nsoftware_spec_uid = client.software_specifications.get_id_by_name(\"vector-index-genai-flow-v2-software-specification-memory\")\n\nif (software_spec_uid == \"Not Found\"):\n    software_spec_uid = client.spaces.promote(\"dc64d3c6-0bf5-4477-9b56-14504753f82c\", \"b43a5e28-8c49-4cc1-94e5-ce92969f6217\", \"a06ae04e-5905-41af-b9cd-f097e02e1f63\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Store the function in the repository\n\nfunction_metadata = {\n    client.repository.FunctionMetaNames.NAME: \"test notebook\",\n    client.repository.FunctionMetaNames.DESCRIPTION: \"\",\n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: software_spec_uid\n}\n\nfunction_details = client.repository.store_function(meta_props=function_metadata, function=my_deployable_chain_function)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Get published function ID\n\nfunction_id = client.repository.get_function_id(function_details)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Deploy the stored function\n\ndeployment_metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"test notebook\",\n    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n    client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { \"name\": \"S\" }\n}\n\nfunction_deployment_details = client.deployments.create(function_id, meta_props=deployment_metadata)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 4. Test deployed function"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Get the endpoint URL of the function deployment just created\n\nfunction_deployment_id = client.deployments.get_uid(function_deployment_details)\nfunction_deployment_endpoint_url = client.deployments.get_scoring_href(function_deployment_details)\nprint(function_deployment_id)\nprint(function_deployment_endpoint_url)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "messages = []\nremote_question = \"Change this question to test your function\"\nmessages.append({ \"role\" : \"user\", \"content\": remote_question })\npayload = {\"input_data\": [{\"fields\": [ \"Search\", \"access_token\" ],\n                            \"values\": [messages, [get_bearer_token()]]\n                            }\n                        ]\n}",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "result = client.deployments.score(function_deployment_id, payload)\nif \"error\" in result:\n    print(result[\"error\"])\nelse:\n    print(result)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Next steps\nYou successfully deployed and tested the RAG chain function! You can now view\nyour deployment and test it as a REST API endpoint.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.10",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}